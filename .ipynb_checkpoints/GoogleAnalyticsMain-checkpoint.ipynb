{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1064a9d",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a312829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pylab\n",
    "import calendar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "from scipy import stats\n",
    "import missingno as msno\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, KBinsDiscretizer, StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_log_error, r2_score\n",
    "import xgboost as xgb\n",
    "from Helper_Functions import degree_search\n",
    "from Helper_Functions import polynomial_search\n",
    "from Helper_Functions import json_to_series\n",
    "from joblib import dump, load\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a77a06f",
   "metadata": {},
   "source": [
    "## Read Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a67abfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Edwin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3441: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "training = pd.read_csv(\"Data/train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83f88aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = training.sample(n=200000).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3aa36f",
   "metadata": {},
   "source": [
    "## Expand Json Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e055c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_cols = ['device', 'geoNetwork', 'totals', 'trafficSource', 'adwordsClickInfo']\n",
    "\n",
    "# training['device'].map(eval, false=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for col in json_cols:\n",
    "    training[col] = training[col].astype(str)\n",
    "    training[col] = training[col].str.replace('false', 'False').str.replace('true', 'True').map(eval)\n",
    "    training = pd.concat([training.drop([col], axis=1), training[col].apply(pd.Series)], axis=1)\n",
    "    \n",
    "training.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a6c9a1",
   "metadata": {},
   "source": [
    "## NA value recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e1fa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in training.columns:\n",
    "    training[col] = training[col].replace('(not set)', np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65b7ed1",
   "metadata": {},
   "source": [
    "## Remove Unavailable Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494745c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols_to_drop = [col for col in training.columns if 'not available in demo dataset' in list(training[col])]\n",
    "print(cols_to_drop)\n",
    "\n",
    "training = training[[col for col in training.columns if col not in cols_to_drop]]\n",
    "\n",
    "training.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19879c36",
   "metadata": {},
   "source": [
    "## Remove columns without information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc47d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# socialEngagementType has all the same values\n",
    "# deviceCategory is perfectly multi-collinear with isMobile\n",
    "# continent, highly multi-collinear with country and subContinent - could potentially run a multicolinearity test with VIF\n",
    "# all visits are 1\n",
    "# keyword, adContent: too sparse to be used\n",
    "# isVideoAd: Collinear with adNetworkType\n",
    "# targetingCriteria: no values\n",
    "cols_to_drop2 = ['socialEngagementType', 'deviceCategory', 'continent', 'visits', 'isTrueDirect', 'keyword', 'adContent', \n",
    "                 'isVideoAd', 'targetingCriteria']\n",
    "training = training[[col for col in training.columns if col not in cols_to_drop2]]\n",
    "\n",
    "training.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c980e332",
   "metadata": {},
   "source": [
    "## Remove irrelevant ID columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f54785",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop3 = ['sessionId', 'visitId', 'gclId']\n",
    "training = training[[col for col in training.columns if col not in cols_to_drop3]]\n",
    "\n",
    "training.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d39ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cols = ['fullVisitorId']\n",
    "\n",
    "date_cols = ['date', 'visitStartTime']\n",
    "# Note: sparsity on network domain is very high\n",
    "cat_cols = ['channelGrouping', 'browser', 'operatingSystem', 'subContinent', 'country', \n",
    "            'networkDomain', 'referralPath', 'campaign', 'page', 'slot', 'adNetworkType', 'medium']\n",
    "dummy_cols = ['isMobile']\n",
    "\n",
    "# For hits: there is a need to group everything beyond 10\n",
    "numeric_cols = ['pageviews', 'bounces', 'newVisits']\n",
    "\n",
    "cols_with_sparsity_issues = ['visitNumber', 'hits', 'referralPath', 'campaign', 'source', 'page', 'slot', 'adNetworkType']\n",
    "\n",
    "dependent_var = ['logTransactionRevenue']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7f9bdb",
   "metadata": {},
   "source": [
    "## Log Transformation of DV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac77d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "training['logTransactionRevenue'] = np.log(training['transactionRevenue'].astype(float))\n",
    "training = training.drop('transactionRevenue', axis=1)\n",
    "training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eae206",
   "metadata": {},
   "source": [
    "## Fill NA (only when NA's reasonably mean the same thing as 0's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a04de83",
   "metadata": {},
   "outputs": [],
   "source": [
    "training['bounces'] = training['bounces'].fillna(0)\n",
    "training['newVisits'] = training['newVisits'].fillna(0)\n",
    "training['logTransactionRevenue'] = training['logTransactionRevenue'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65037698",
   "metadata": {},
   "source": [
    "## Sparse column value grouping into Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbbc60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in list(set(cols_with_sparsity_issues+cat_cols)):\n",
    "    s = training[col].value_counts()\n",
    "    training[col] = np.where(training[col].isin(s.index[s < 200]), 'Other', training[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2d7f17",
   "metadata": {},
   "source": [
    "## Datetime Object Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22ee2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have to figure out what timestamp format is the visitStartTime using, currently with the standard 10 digit timestamp it just maps\n",
    "# to 1970's\n",
    "\n",
    "training[\"weekday\"] = training.date.apply(lambda dateString : calendar.day_name[datetime.strptime(str(dateString),\"%Y%m%d\").weekday()])\n",
    "training[\"month\"] = training.date.apply(lambda dateString : calendar.month_name[datetime.strptime(str(dateString),\"%Y%m%d\").month])\n",
    "training[\"year\"] = training.date.apply(lambda dateString : str(datetime.strptime(str(dateString),\"%Y%m%d\").year))\n",
    "training = training.drop('date',axis=1)\n",
    "# training[\"datetime\"] = training.date.apply(lambda dateString : str(datetime.fromtimestamp(dateString)))\n",
    "# training[\"hour\"] = training.datetime.apply(lambda x : x.split()[1].split(\":\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbec8dc",
   "metadata": {},
   "source": [
    "## Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e58bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(training,figsize=(12,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac6ed3f",
   "metadata": {},
   "source": [
    "## Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d88460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove data greater than 3 std away from mean\n",
    "\n",
    "print(\"before outlier removal, \" + str(training.shape[0]) + \" rows\")\n",
    "\n",
    "for col in numeric_cols:\n",
    "    training[col] = training[col].astype(float)\n",
    "    training = training[np.abs(training[col]-training[col].mean())<=(3*training[col].std())] \n",
    "    \n",
    "print(\"after outlier removal, \" + str(training.shape[0]) + \" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4ecc4b",
   "metadata": {},
   "source": [
    "## Get Dummies & Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937e3a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(training[cat_cols+['weekday','month','year']], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aa2efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = training.join(pd.get_dummies(training[cat_cols+['weekday','month','year']], drop_first=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184af25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = training[[col for col in training.columns if col not in list(set(cols_with_sparsity_issues+cat_cols+['weekday','month','year']))]]\n",
    "training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5192b3",
   "metadata": {},
   "source": [
    "## Other individual column adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4859418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training['isMobile'] = training['isMobile'].astype(int)\n",
    "training = training.drop('visitStartTime', axis=1)\n",
    "training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b9db6c",
   "metadata": {},
   "source": [
    "## Numeric Feature Scaling - especially for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018869ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# training['humidity'] = scaler.fit_transform(training[['humidity']])\n",
    "# training['atemp'] = scaler.fit_transform(training[['atemp']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb3b579",
   "metadata": {},
   "source": [
    "## Correlation Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7f451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrMatt = training[[col for col in training if col!='fullVisitorId']].corr()\n",
    "mask = np.array(corrMatt)\n",
    "mask[np.tril_indices_from(mask)] = False\n",
    "fig,ax= plt.subplots()\n",
    "fig.set_size_inches(50,20)\n",
    "sn.heatmap(corrMatt, mask=mask,vmax=.8, square=True,annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c8faac",
   "metadata": {},
   "source": [
    "## Degree Search - uneeded as we are using gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6b3479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df, degree_dict = degree_search(df=training[['atemp', 'temp', 'windspeed', 'humidity','count']], dep='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1508d88d",
   "metadata": {},
   "source": [
    "## Dependent & Independent Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92556ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training[[col for col in training.columns if col not in dependent_var and col not in id_cols]]\n",
    "y = training[dependent_var[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a81519a",
   "metadata": {},
   "source": [
    "## Polynomial Degree Search - uneeded as we are using gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3759e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poly_dict = polynomial_search(X, y, highest_degree=2)\n",
    "# poly_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8530d65",
   "metadata": {},
   "source": [
    "## Generate Polynomial Features for Numeric Variables  - uneeded as we are using gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8b5e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poly = PolynomialFeatures(1)\n",
    "# X = poly.fit_transform(X)\n",
    "\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c39cd3",
   "metadata": {},
   "source": [
    "## Feature Selection - Variance Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f5dbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = VarianceThreshold(threshold=(.91 * (1 - .91)))\n",
    "X = sel.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c586f9",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0042bcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "X, y, test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b8ba89",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ffa25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lreg = LinearRegression()\n",
    "lreg.fit(X_train, y_train)\n",
    "\n",
    "lreg.score(X_test, y_test)\n",
    "\n",
    "predictions = lreg.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "txt1 = \"Adjusted R squared: {AR}; \\nRMSLE: {rmsle}\".format(AR=lreg.score(X_test, y_test), \n",
    "                                                            rmsle=np.sqrt(mean_squared_log_error(y_test, np.maximum(predictions, 0))))\n",
    "print(txt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc45c6ae",
   "metadata": {},
   "source": [
    "## Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd4f88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "svreg = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\n",
    "svreg.fit(X_train, y_train)\n",
    "\n",
    "svreg.score(X_test, y_test)\n",
    "\n",
    "predictions = svreg.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "txt1 = \"Adjusted R squared: {AR}; \\nRMSLE: {rmsle}\".format(AR=svreg.score(X_test, y_test), \n",
    "                                                            rmsle=np.sqrt(mean_squared_log_error(y_test, np.maximum(predictions, 0))))\n",
    "print(txt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3267e7d7",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb657bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfreg = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "rfreg.fit(X_train, y_train)\n",
    "\n",
    "rfreg.score(X_test, y_test)\n",
    "\n",
    "predictions = rfreg.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "txt1 = \"Adjusted R squared: {AR}; \\nRMSLE: {rmsle}\".format(AR=rfreg.score(X_test, y_test), \n",
    "                                                            rmsle=np.sqrt(mean_squared_log_error(y_test, np.maximum(predictions, 0))))\n",
    "print(txt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4025e6",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e72c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "# Training with random_grid\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "ar = rf_random.score(X_test, y_test)\n",
    "\n",
    "predictions = rf_random.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "txt1 = \"Adjusted R squared: {AR}; \\nRMSLE: {rmsle}\".format(AR=ar, \n",
    "                                                            rmsle=np.sqrt(mean_squared_log_error(y_test, np.maximum(predictions, 0))))\n",
    "print(txt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3ed500",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5d16be",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbreg = GradientBoostingRegressor(random_state=0)\n",
    "gbreg.fit(X_train, y_train)\n",
    "\n",
    "gbreg.score(X_test, y_test)\n",
    "\n",
    "predictions = gbreg.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "txt1 = \"Adjusted R squared: {AR}; \\nRMSLE: {rmsle}\".format(AR=gbreg.score(X_test, y_test), \n",
    "                                                            rmsle=np.sqrt(mean_squared_log_error(y_test, np.maximum(predictions, 0))))\n",
    "print(txt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8392bfdc",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cee5c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in gbm\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "# Training with random_grid\n",
    "\n",
    "gb = GradientBoostingRegressor(random_state=0)\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "gb_random = RandomizedSearchCV(estimator = gb, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "gb_random.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "ar = gb_random.score(X_test, y_test)\n",
    "\n",
    "predictions = gb_random.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "txt1 = \"Adjusted R squared: {AR}; \\nRMSLE: {rmsle}\".format(AR=ar, \n",
    "                                                            rmsle=np.sqrt(mean_squared_log_error(y_test, np.maximum(predictions, 0))))\n",
    "print(txt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5247574a",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7ca202",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = xgb.DMatrix(X_train, y_train)\n",
    "test_data = xgb.DMatrix(X_test, y_test)\n",
    "\n",
    "params = {\"booster\": 'gbtree',\n",
    "              \"objective\" : \"reg:squaredlogerror\",\n",
    "              \"eval_metric\" : \"auc\", \n",
    "              \"is_unbalance\": True,\n",
    "              \"n_estimators\": 500,\n",
    "              \"max_depth\" : 5,\n",
    "              \"reg_alpha\" : 0.01,\n",
    "              \"reg_lambda\" : 0.01,\n",
    "              \"gamma\": 5,\n",
    "              \"num_threads\" : 20,\n",
    "              \"min_child_weight\" : 5,\n",
    "              \"learning_rate\" : 0.01,\n",
    "              \"subsample_freq\" : 5,\n",
    "              \"seed\" : 42,\n",
    "              \"verbosity\" : 0,\n",
    "              \"num_boost_round\": 500}\n",
    "\n",
    "\n",
    "\n",
    "cv_result = xgb.cv(params,\n",
    "                   train_data,\n",
    "                   1000,\n",
    "                   early_stopping_rounds=100,\n",
    "                   stratified=True,\n",
    "                   nfold=3)\n",
    "\n",
    "xgboost = xgb.train(params, train_data, 10000)\n",
    "\n",
    "predictions = xgboost.predict(test_data)\n",
    "\n",
    "ar = r2_score(y_test,np.maximum(predictions, 0))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "txt1 = \"Adjusted R squared: {AR}; \\nRMSLE: {rmsle}\".format(AR=ar, \n",
    "                                                            rmsle=np.sqrt(mean_squared_log_error(y_test, np.maximum(predictions, 0))))\n",
    "print(txt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f224cd1",
   "metadata": {},
   "source": [
    "## Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337b79e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(gbreg, 'Models/GradientBoosting.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445e5462",
   "metadata": {},
   "source": [
    "## Generate Kaggle Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f81c7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('Data/test.csv')\n",
    "\n",
    "\n",
    "\n",
    "test_data[\"date\"] = test_data.datetime.apply(lambda x : x.split()[0])\n",
    "test_data[\"hour\"] = test_data.datetime.apply(lambda x : x.split()[1].split(\":\")[0])\n",
    "test_data[\"weekday\"] = test_data.date.apply(lambda dateString : calendar.day_name[datetime.strptime(dateString,\"%Y-%m-%d\").weekday()])\n",
    "test_data[\"month\"] = test_data.date.apply(lambda dateString : calendar.month_name[datetime.strptime(dateString,\"%Y-%m-%d\").month])\n",
    "\n",
    "\n",
    "# Mapping to strings\n",
    "test_data[\"season\"] = test_data.season.map({1: \"Spring\", 2 : \"Summer\", 3 : \"Fall\", 4 :\"Winter\" })\n",
    "test_data[\"weather\"] = test_data.weather.map({1: \" Clear + Few clouds + Partly cloudy + Partly cloudy\",\\\n",
    "                                        2 : \" Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \", \\\n",
    "                                        3 : \" Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\", \\\n",
    "                                        4 :\" Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \" })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Mapping to categories\n",
    "\n",
    "categoryVariables = [\"season\",\"weather\", \"weekday\"]\n",
    "textCategories = [\"season\",\"weather\", \"weekday\"]\n",
    "alreadyCategories = [\"holiday\", \"workingday\"]\n",
    "binVariables = [\"hour\"]\n",
    "\n",
    "\n",
    "for var in categoryVariables:\n",
    "    test_data[var] = test_data[var].astype(\"category\")\n",
    "    \n",
    "    \n",
    "    \n",
    "# Unrepeatable\n",
    "bins = KBinsDiscretizer(n_bins=7, encode='ordinal', strategy='uniform')\n",
    "test_data[binVariables] = bins.fit_transform(test_data[binVariables])\n",
    "test_data[binVariables] = test_data[binVariables].astype(\"category\")\n",
    "\n",
    "\n",
    "dummie_cols = pd.get_dummies(test_data[textCategories+binVariables]).columns\n",
    "test_data = test_data.join(pd.get_dummies(test_data[textCategories+binVariables]))\n",
    "\n",
    "X = test_data[numericVariables+list(dummie_cols)+alreadyCategories]\n",
    "\n",
    "\n",
    "X = poly.transform(X)\n",
    "X = sel.transform(X)\n",
    "\n",
    "test_data['count'] = rf_random.predict(X)\n",
    "\n",
    "test_data.to_csv('Data/test_labelled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41d80f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[['datetime', 'count']].to_csv('Data/test_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62051eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[['datetime', 'count']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
